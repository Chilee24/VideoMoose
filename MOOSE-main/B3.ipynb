{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bài Tập 3: Mạng Nơ Ron Một Lớp Ẩn Cho Nhận Dạng Chữ Số MNIST\n",
    "\n",
    "Giả sử ta có một mạng nơ ron gồm:\n",
    "- **Input layer:** 784 đặc trưng (mỗi ảnh được phẳng thành vector 784 chiều).\n",
    "- **Lớp ẩn:** 30 nơ ron, sử dụng hàm kích hoạt sigmoid.\n",
    "- **Lớp output:** 10 nơ ron (cho 10 lớp chữ số 0-9), sử dụng hàm kích hoạt sigmoid.\n",
    "- **Hàm Loss:** Cross-Entropy (áp dụng cho từng đầu ra, tính trung bình qua $m$ mẫu).\n",
    "\n",
    "Trong các phần dưới đây, ta sẽ trình bày:\n",
    "1. Phương trình forward (tính lan truyền).\n",
    "2. Tính đạo hàm (Back-Propagation) dưới dạng ma trận.\n",
    "3. Các bước của thuật toán Gradient Descent.\n",
    "\n",
    "---\n",
    "\n",
    "## Câu 1: Phương Trình Forward\n",
    "\n",
    "Giả sử ta có tập dữ liệu gồm $m$ mẫu:\n",
    "- **Input:** $ X \\in \\mathbb{R}^{784 \\times m} $\n",
    "\n",
    "**Lớp ẩn (Hidden Layer):**\n",
    "- Trọng số: $ W^{[1]} \\in \\mathbb{R}^{30 \\times 784} $\n",
    "- Bias: $ b^{[1]} \\in \\mathbb{R}^{30 \\times 1} $\n",
    "- Tính tổng tuyến tính:\n",
    "  $\n",
    "  Z^{[1]} = W^{[1]} X + b^{[1]} \\quad \\in \\mathbb{R}^{30 \\times m}\n",
    "  $\n",
    "- Áp dụng hàm kích hoạt sigmoid:\n",
    "  $\n",
    "  A^{[1]} = \\sigma\\bigl(Z^{[1]}\\bigr) = \\frac{1}{1 + e^{-{Z^{[1]}}}} \\quad \\in \\mathbb{R}^{30 \\times m}\n",
    "  $\n",
    "\n",
    "**Lớp output (Output Layer):**\n",
    "- Trọng số: $ W^{[2]} \\in \\mathbb{R}^{10 \\times 30} $\n",
    "- Bias: $ b^{[2]} \\in \\mathbb{R}^{10 \\times 1} $\n",
    "- Tính tổng tuyến tính:\n",
    "  $\n",
    "  Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} \\quad \\in \\mathbb{R}^{10 \\times m}\n",
    "  $\n",
    "- Áp dụng hàm kích hoạt sigmoid:\n",
    "  $\n",
    "  A^{[2]} = \\sigma\\bigl(Z^{[2]}\\bigr) = \\frac{1}{1 + e^{-{Z^{[2]}}}} \\quad \\in \\mathbb{R}^{10 \\times m}\n",
    "  $\n",
    "\n",
    "**Hàm Loss (Cross-Entropy):**  \n",
    "Giả sử nhãn đúng được mã hóa one-hot, ta có:\n",
    "- $ Y \\in \\mathbb{R}^{10 \\times m} $ với $ m $ là số mẫu.\n",
    "- $ A^{[2]} \\in \\mathbb{R}^{10 \\times m} $ là đầu ra của lớp output (sau khi áp dụng hàm sigmoid).\n",
    "\n",
    "Hàm loss cho mỗi mẫu $ i $ được tính theo công thức:\n",
    "$\n",
    "L^{(i)} = -\\sum_{j=1}^{10} \\Bigl[ Y_{j}^{(i)} \\log\\Bigl( A_{j}^{[2](i)} \\Bigr) + \\Bigl( 1 - Y_{j}^{(i)} \\Bigr) \\log\\Bigl( 1 - A_{j}^{[2](i)} \\Bigr) \\Bigr]\n",
    "$\n",
    "\n",
    "Hàm loss trung bình trên toàn bộ tập mẫu là:\n",
    "$\n",
    "J = \\frac{1}{m} \\sum_{i=1}^{m} L^{(i)}\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "## Câu 2: Tính Đạo Hàm \n",
    "\n",
    "### 1. Đạo hàm Ở Lớp Output\n",
    "\n",
    "- **Đầu ra của lớp output:**\n",
    "  $\n",
    "  A^{[2]} = \\sigma\\bigl(Z^{[2]}\\bigr)\n",
    "  $\n",
    "- Với hàm sigmoid kết hợp với Cross-Entropy, ta có đạo hàm theo từng mẫu:\n",
    "  $\n",
    "  \\frac{\\partial L^{(i)}}{\\partial Z^{[2](i)}} = A^{[2](i)} - Y^{(i)}\n",
    "  $\n",
    "- Dạng ma trận cho toàn bộ tập mẫu:\n",
    "  $\n",
    "  dZ^{[2]} = A^{[2]} - Y \\quad \\in \\mathbb{R}^{10 \\times m}\n",
    "  $\n",
    "- Tính đạo hàm theo $W^{[2]}$ và $b^{[2]}$:\n",
    "  $\n",
    "  dW^{[2]} = \\frac{1}{m} \\, dZ^{[2]} (A^{[1]})^T \\quad \\in \\mathbb{R}^{10 \\times 30}\n",
    "  $\n",
    "  $\n",
    "  db^{[2]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[2](i)} \\quad \\in \\mathbb{R}^{10 \\times 1}\n",
    "  $\n",
    "\n",
    "### 2. Đạo hàm Ở Lớp Ẩn\n",
    "\n",
    "**Bước 1: Lan Truyền Lỗi Về Lớp Ẩn**\n",
    "\n",
    "- Lỗi từ lớp output được lan truyền về lớp ẩn thông qua trọng số:\n",
    "  $\n",
    "  dA^{[1]} = (W^{[2]})^T dZ^{[2]} \\quad \\in \\mathbb{R}^{30 \\times m}\n",
    "  $\n",
    "\n",
    "**Bước 2: Tính Đạo Hàm của Hàm Kích Hoạt Sigmoid**\n",
    "\n",
    "- Hàm kích hoạt của lớp ẩn:\n",
    "  $\n",
    "  A^{[1]} = \\sigma\\bigl(Z^{[1]}\\bigr)\n",
    "  $\n",
    "- Đạo hàm của sigmoid theo $Z^{[1]}$ (theo từng phần tử):\n",
    "  $\n",
    "  \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} = A^{[1]} \\circ (1-A^{[1]})\n",
    "  $\n",
    "  (Ở đây $\\circ$ biểu thị phép nhân phần tử.)\n",
    "\n",
    "**Bước 3: Tính $dZ^{[1]}$ cho Lớp Ẩn**\n",
    "\n",
    "- Áp dụng quy tắc chuỗi:\n",
    "  $\n",
    "  dZ^{[1]} = dA^{[1]} \\circ \\Bigl(A^{[1]} \\circ (1-A^{[1]})\\Bigr)\\quad \\in \\mathbb{R}^{30 \\times m}\n",
    "  $\n",
    "- \n",
    "  Hay, với mỗi phần tử:\n",
    "  $\n",
    "  [dZ^{[1]}]_{ij} = [dA^{[1]}]_{ij} \\times [A^{[1]}]_{ij} \\times \\bigl(1-[A^{[1]}]_{ij}\\bigr)\n",
    "  $\n",
    "\n",
    "**Bước 4: Tính Đạo Hàm Theo $W^{[1]}$ và $b^{[1]}$**\n",
    "\n",
    "- Đạo hàm theo $W^{[1]}$:\n",
    "  $\n",
    "  dW^{[1]} = \\frac{1}{m} \\, dZ^{[1]} \\, X^T \\quad \\in \\mathbb{R}^{30 \\times 784}\n",
    "  $\n",
    "- Đạo hàm theo $b^{[1]}$:\n",
    "  $\n",
    "  db^{[1]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[1](i)} \\quad \\in \\mathbb{R}^{30 \\times 1}\n",
    "  $\n",
    "  (Tổng các cột của $dZ^{[1]}$.)\n",
    "\n",
    "---\n",
    "\n",
    "## Câu 3: Mô Tả Các Bước Của Thuật Toán Gradient Descent\n",
    "\n",
    "Quy trình huấn luyện bằng Gradient Descent bao gồm các bước sau:\n",
    "\n",
    "1. **Khởi tạo tham số:**\n",
    "   - Khởi tạo $ W^{[1]} \\in \\mathbb{R}^{30 \\times 784} $ và $ b^{[1]} \\in \\mathbb{R}^{30 \\times 1} $ (có thể khởi tạo ngẫu nhiên).\n",
    "   - Khởi tạo $ W^{[2]} \\in \\mathbb{R}^{10 \\times 30} $ và $ b^{[2]} \\in \\mathbb{R}^{10 \\times 1} $.\n",
    "\n",
    "2. **Forward Propagation:**\n",
    "   - Tính $ Z^{[1]} = W^{[1]} X + b^{[1]} $.\n",
    "   - Tính $ A^{[1]} = \\sigma\\bigl(Z^{[1]}\\bigr) $.\n",
    "   - Tính $ Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]} $.\n",
    "   - Tính $ A^{[2]} = \\sigma\\bigl(Z^{[2]}\\bigr) $.\n",
    "\n",
    "3. **Tính Loss:**\n",
    "   - Tính hàm loss Cross-Entropy trên toàn bộ tập mẫu:\n",
    "$\n",
    "J = -\\frac{1}{m} \\sum_{i=1}^{m} \\sum_{j=1}^{10} \\Bigl[ Y_{j}^{(i)} \\log\\Bigl(A_{j}^{[2](i)}\\Bigr) + \\Bigl(1-Y_{j}^{(i)}\\Bigr) \\log\\Bigl(1-A_{j}^{[2](i)}\\Bigr) \\Bigr]\n",
    "$\n",
    "\n",
    "4. **Backward Propagation:**\n",
    "   - **Lớp Output:**\n",
    "     - Tính $ dZ^{[2]} = A^{[2]} - Y $.\n",
    "     - Tính $ dW^{[2]} = \\frac{1}{m} \\, dZ^{[2]} (A^{[1]})^T $.\n",
    "     - Tính $ db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]} $.\n",
    "   - **Lớp Ẩn:**\n",
    "     - Lan truyền lỗi: $ dA^{[1]} = (W^{[2]})^T dZ^{[2]} $.\n",
    "     - Tính $ dZ^{[1]} = dA^{[1]} \\circ \\Bigl(A^{[1]} \\circ (1-A^{[1]})\\Bigr) $.\n",
    "     - Tính $ dW^{[1]} = \\frac{1}{m} \\, dZ^{[1]} \\, X^T $.\n",
    "     - Tính $ db^{[1]} = \\frac{1}{m} \\sum dZ^{[1]} $.\n",
    "\n",
    "5. **Cập nhật Tham Số:**\n",
    "   - Cập nhật các tham số theo công thức:\n",
    "     $\n",
    "     W^{[l]} := W^{[l]} - \\alpha \\, dW^{[l]} \\quad \\text{với } l=1,2\n",
    "     $\n",
    "     $\n",
    "     b^{[l]} := b^{[l]} - \\alpha \\, db^{[l]} \\quad \\text{với } l=1,2\n",
    "     $\n",
    "   - Ở đây $\\alpha$ là learning rate.\n",
    "\n",
    "6. **Lặp Lại:**\n",
    "   - Thực hiện các bước 2–5 trong số vòng lặp (iterations) cho đến khi hàm loss hội tụ hoặc đạt số vòng lặp tối đa.\n",
    "\n",
    "\n"
   ],
   "id": "36f48158f92e6ae0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-01T13:07:19.095770Z",
     "start_time": "2025-03-01T13:07:18.543724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    # Lớp ẩn\n",
    "    Z1 = np.dot(W1, X) + b1        # (30, m)\n",
    "    A1 = sigmoid(Z1)               # (30, m)\n",
    "    # Lớp output\n",
    "    Z2 = np.dot(W2, A1) + b2       # (10, m)\n",
    "    A2 = sigmoid(Z2)               # (10, m)\n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "def compute_loss(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = - (1/m) * np.sum(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    return loss\n",
    "\n",
    "def backward_propagation(X, Y, cache, W2):\n",
    "    m = X.shape[1]\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    # Lớp output\n",
    "    dZ2 = A2 - Y                              # (10, m)\n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)             # (10, 30)\n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)  # (10, 1)\n",
    "    # Lớp ẩn\n",
    "    dA1 = np.dot(W2.T, dZ2)                     # (30, m)\n",
    "    dZ1 = dA1 * (A1 * (1 - A1))                 # (30, m)\n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)              # (30, 784)\n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)  # (30, 1)\n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1, \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "def update_parameters(W1, b1, W2, b2, grads, learning_rate):\n",
    "    W1 = W1 - learning_rate * grads[\"dW1\"]\n",
    "    b1 = b1 - learning_rate * grads[\"db1\"]\n",
    "    W2 = W2 - learning_rate * grads[\"dW2\"]\n",
    "    b2 = b2 - learning_rate * grads[\"db2\"]\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def model(X, Y, num_iterations=10000, learning_rate=0.1, print_cost=True):\n",
    "    np.random.seed(1)\n",
    "    m = X.shape[1]\n",
    "    # Khởi tạo tham số\n",
    "    W1 = np.random.randn(30, 784) * 0.01\n",
    "    b1 = np.zeros((30, 1))\n",
    "    W2 = np.random.randn(10, 30) * 0.01\n",
    "    b2 = np.zeros((10, 1))\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_propagation(X, W1, b1, W2, b2)\n",
    "        cost = compute_loss(A2, Y)\n",
    "        grads = backward_propagation(X, Y, cache, W2)\n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, grads, learning_rate)\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(f\"Iteration {i}, cost: {cost:.6f}\")\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters\n",
    "\n",
    "# Giả sử X là ma trận dữ liệu có kích thước (784, m)\n",
    "# Và Y là ma trận one-hot có kích thước (10, m)\n",
    "# Sau đó gọi:\n",
    "# parameters = model(X, Y, num_iterations=10000, learning_rate=0.1)"
   ],
   "id": "77bad96ad994d59e",
   "outputs": [],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
