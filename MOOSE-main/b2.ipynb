{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bài Tập 2: Giải Bài Toán XOR\n",
    "\n",
    "Xây dựng một mạng neural đơn giản với:\n",
    "- **Input:** 2 features (x₁, x₂)\n",
    "- **Lớp ẩn:** 2 nơ ron, sử dụng hàm kích hoạt sigmoid\n",
    "- **Output:** 1 nơ ron, sử dụng hàm kích hoạt sigmoid\n",
    "- **Hàm Loss:** Cross-Entropy\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Câu 1: Phương Trình Forward\n",
    "\n",
    "Giả sử ta có:\n",
    "- **Input:** $X$ có kích thước (2, m) với $m$ là số mẫu.\n",
    "- **Lớp ẩn:** \n",
    "  - Trọng số $W^{[1]}$ có kích thước (2, 2) (2 nơ ron, mỗi nơ ron nhận 2 đầu vào).\n",
    "  - Bias $b^{[1]}$ có kích thước (2, 1).\n",
    "  - Tính tổng tuyến tính cho lớp ẩn:\n",
    "    $\n",
    "    Z^{[1]} = W^{[1]} X + b^{[1]}\n",
    "    $\n",
    "  - Áp dụng hàm kích hoạt sigmoid:\n",
    "    $\n",
    "    A^{[1]} = \\sigma\\bigl(Z^{[1]}\\bigr) = \\frac{1}{1 + e^{-Z^{[1]}}}\n",
    "    $\n",
    "  \n",
    "- **Lớp Output:**\n",
    "  - Trọng số $W^{[2]}$ có kích thước (1, 2) (1 nơ ron, nhận đầu ra từ 2 nơ ron lớp ẩn).\n",
    "  - Bias $b^{[2]}$ là một số vô hướng.\n",
    "  - Tính tổng tuyến tính cho lớp output:\n",
    "    $\n",
    "    Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}\n",
    "    $\n",
    "  - Áp dụng hàm sigmoid:\n",
    "    $\n",
    "    A^{[2]} = \\sigma\\bigl(Z^{[2]}\\bigr) = \\frac{1}{1 + e^{-Z^{[2]}}}\n",
    "    $\n",
    "  \n",
    "- **Hàm Loss (cho mỗi mẫu):**  \n",
    "  Với nhãn đúng $y$ và dự đoán $A^{[2]}$, hàm Cross-Entropy:\n",
    "  $\n",
    "  L = -\\Bigl[ y\\log\\bigl(A^{[2]}\\bigr) + (1-y)\\log\\bigl(1-A^{[2]}\\bigr) \\Bigr]\n",
    "  $\n",
    "\n",
    "  Và với $m$ mẫu, hàm loss trung bình:\n",
    "  $\n",
    "  J = \\frac{1}{m} \\sum_{i=1}^{m} L^{(i)}\n",
    "  $\n",
    "\n",
    "---\n",
    "\n",
    "# Câu 2: Đạo Hàm (Back-Propagation)\n",
    "\n",
    "## 1. Đạo Hàm Ở Lớp Output\n",
    "\n",
    "### a. Tính $ \\frac{\\partial J}{\\partial Z^{[2]}} $\n",
    "\n",
    "Với hàm sigmoid và hàm loss Cross-Entropy, ta có kết quả đã được chứng minh rằng (dưới dạng phần tử):\n",
    "$\n",
    "\\frac{\\partial J^{(i)}}{\\partial Z^{[2](i)}} = A^{[2](i)} - y^{(i)}\n",
    "$\n",
    "Chuyển sang dạng ma trận cho toàn bộ $m$ mẫu:\n",
    "$\n",
    "dZ^{[2]} = A^{[2]} - Y \\quad \\in \\mathbb{R}^{1 \\times m}\n",
    "$\n",
    "\n",
    "### b. Tính đạo hàm theo $W^{[2]}$ và $b^{[2]}$\n",
    "\n",
    "Áp dụng quy tắc chuỗi và tính trung bình qua $m$ mẫu:\n",
    "\n",
    "- Đạo hàm theo $W^{[2]}$:\n",
    "  $\n",
    "  dW^{[2]} = \\frac{1}{m} \\, dZ^{[2]} \\, (A^{[1]})^T \\quad \\in \\mathbb{R}^{1 \\times 2}\n",
    "  $\n",
    "- Đạo hàm theo $b^{[2]}$:\n",
    "  $\n",
    "  db^{[2]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[2](i)} \\quad \\in \\mathbb{R}^{1 \\times 1}\n",
    "  $\n",
    "  (Tổng theo các cột của $dZ^{[2]}$.)\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Đạo Hàm Ở Lớp Ẩn\n",
    "\n",
    "### a. Lan Truyền Lỗi Về Lớp Ẩn\n",
    "\n",
    "Từ lớp output, lỗi được lan truyền về lớp ẩn:\n",
    "$\n",
    "dA^{[1]} = (W^{[2]})^T \\, dZ^{[2]} \\quad \\in \\mathbb{R}^{2 \\times m}\n",
    "$\n",
    "\n",
    "### b. Tính $ \\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} $\n",
    "\n",
    "Hàm kích hoạt sigmoid có đạo hàm (theo từng phần tử):\n",
    "$\n",
    "\\frac{\\partial A^{[1]}}{\\partial Z^{[1]}} = A^{[1]} \\circ (1 - A^{[1]})\n",
    "$\n",
    "Trong đó $\\circ$ biểu thị phép nhân phần tử (Hadamard product).\n",
    "\n",
    "### c. Tính $ \\frac{\\partial J}{\\partial Z^{[1]}} $\n",
    "\n",
    "Áp dụng quy tắc chuỗi:\n",
    "$\n",
    "dZ^{[1]} = dA^{[1]} \\circ \\Bigl( A^{[1]} \\circ (1 - A^{[1]}) \\Bigr) \\quad \\in \\mathbb{R}^{2 \\times m}\n",
    "$\n",
    "\n",
    "### d. Tính đạo hàm theo $W^{[1]}$ và $b^{[1]}$\n",
    "\n",
    "- Đạo hàm theo $W^{[1]}$:\n",
    "  $\n",
    "  dW^{[1]} = \\frac{1}{m} \\, dZ^{[1]} \\, X^T \\quad \\in \\mathbb{R}^{2 \\times 2}\n",
    "  $\n",
    "- Đạo hàm theo $b^{[1]}$:\n",
    "  $\n",
    "  db^{[1]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[1](i)} \\quad \\in \\mathbb{R}^{2 \\times 1}\n",
    "  $\n",
    "  (Tổng theo các cột của $dZ^{[1]}$.)\n",
    "\n",
    "---\n",
    "\n",
    "## Tổng Hợp\n",
    "\n",
    "- **Lớp Output:**\n",
    "  - $ dZ^{[2]} = A^{[2]} - Y $\n",
    "  - $ dW^{[2]} = \\frac{1}{m} \\, dZ^{[2]} (A^{[1]})^T $\n",
    "  - $ db^{[2]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[2](i)} $\n",
    "\n",
    "- **Lớp Ẩn:**\n",
    "  - $ dA^{[1]} = (W^{[2]})^T \\, dZ^{[2]} $\n",
    "  - $ dZ^{[1]} = dA^{[1]} \\circ \\Bigl(A^{[1]} \\circ (1 - A^{[1]})\\Bigr) $\n",
    "  - $ dW^{[1]} = \\frac{1}{m} \\, dZ^{[1]} \\, X^T $\n",
    "  - $ db^{[1]} = \\frac{1}{m} \\sum_{i=1}^{m} dZ^{[1](i)} $\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Câu 3: Các Bước Thuật Toán Gradient Descent\n",
    "\n",
    "Quy trình học của mạng neural với Gradient Descent bao gồm các bước sau:\n",
    "\n",
    "1. **Khởi tạo tham số:**\n",
    "   - Khởi tạo ngẫu nhiên hoặc bằng 0 cho:\n",
    "     - $W^{[1]}$ (kích thước (2, 2))\n",
    "     - $b^{[1]}$ (kích thước (2, 1))\n",
    "     - $W^{[2]}$ (kích thước (1, 2))\n",
    "     - $b^{[2]}$ (số vô hướng)\n",
    "\n",
    "2. **Forward Propagation:**\n",
    "   - Tính $Z^{[1]} = W^{[1]}X + b^{[1]}$\n",
    "   - Tính $A^{[1]} = \\sigma(Z^{[1]})$\n",
    "   - Tính $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$\n",
    "   - Tính $A^{[2]} = \\sigma(Z^{[2]})$\n",
    "\n",
    "3. **Tính Loss:**\n",
    "   - Tính hàm loss Cross-Entropy cho mỗi mẫu và trung bình lại:\n",
    "     $\n",
    "     J = \\frac{1}{m} \\sum_{i=1}^{m} \\left[-y^{(i)}\\log\\bigl(A^{[2](i)}\\bigr) - \\bigl(1-y^{(i)}\\bigr)\\log\\bigl(1-A^{[2](i)}\\bigr)\\right]\n",
    "     $\n",
    "\n",
    "4. **Backward Propagation:**\n",
    "   - Tính $dZ^{[2]} = A^{[2]} - y$\n",
    "   - Tính $dW^{[2]}$ và $db^{[2]}$\n",
    "   - Tính $dA^{[1]} = (W^{[2]})^T dZ^{[2]}$\n",
    "   - Tính $dZ^{[1]} = dA^{[1]} \\circ \\Bigl(A^{[1]} \\circ (1-A^{[1]})\\Bigr)$\n",
    "   - Tính $dW^{[1]}$ và $db^{[1]}$\n",
    "\n",
    "5. **Cập nhật tham số:**\n",
    "   - Cập nhật các tham số với learning rate $\\alpha$:\n",
    "     $\n",
    "     W^{[l]} := W^{[l]} - \\alpha \\, dW^{[l]} \\quad (l=1,2)\n",
    "     $\n",
    "     $\n",
    "     b^{[l]} := b^{[l]} - \\alpha \\, db^{[l]} \\quad (l=1,2)\n",
    "     $\n",
    "\n",
    "6. **Lặp lại:**  \n",
    "   Lặp lại các bước 2 đến 5 cho đến khi hội tụ (số vòng lặp cố định hoặc loss giảm dưới một ngưỡng nhất định).\n",
    "\n"
   ],
   "id": "b01c5ea5ead9b3cd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    Z1 = np.dot(W1, X) + b1      \n",
    "    A1 = sigmoid(Z1) \n",
    "    # Lớp output\n",
    "    Z2 = np.dot(W2, A1) + b2    \n",
    "    A2 = sigmoid(Z2)     \n",
    "    cache = {\"Z1\": Z1, \"A1\": A1, \"Z2\": Z2, \"A2\": A2}\n",
    "    return A2, cache\n",
    "\n",
    "# Hàm tính loss \n",
    "def compute_loss(A2, Y):\n",
    "    m = Y.shape[1]\n",
    "    loss = - (1/m) * np.sum(Y * np.log(A2 + 1e-8) + (1 - Y) * np.log(1 - A2 + 1e-8))\n",
    "    return loss\n",
    "\n",
    "# Hàm backward \n",
    "def backward_propagation(X, Y, cache, W2):\n",
    "    m = X.shape[1]\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "    \n",
    "    # Lớp output\n",
    "    dZ2 = A2 - Y                       \n",
    "    dW2 = (1/m) * np.dot(dZ2, A1.T)      \n",
    "    db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True) \n",
    "    \n",
    "    # Lớp ẩn\n",
    "    dA1 = np.dot(W2.T, dZ2)   \n",
    "    dZ1 = dA1 * sigmoid_derivative(A1)  \n",
    "    dW1 = (1/m) * np.dot(dZ1, X.T)       \n",
    "    db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True) \n",
    "    \n",
    "    grads = {\"dW1\": dW1, \"db1\": db1,\n",
    "             \"dW2\": dW2, \"db2\": db2}\n",
    "    return grads\n",
    "\n",
    "# Hàm cập nhật tham số\n",
    "def update_parameters(W1, b1, W2, b2, grads, learning_rate):\n",
    "    W1 = W1 - learning_rate * grads[\"dW1\"]\n",
    "    b1 = b1 - learning_rate * grads[\"db1\"]\n",
    "    W2 = W2 - learning_rate * grads[\"dW2\"]\n",
    "    b2 = b2 - learning_rate * grads[\"db2\"]\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def model(X, Y, num_iterations=10000, learning_rate=0.5, print_cost=True):\n",
    "    np.random.seed(42)\n",
    "    m = X.shape[1]\n",
    "    W1 = np.random.uniform(-1, 1, (2, 2))\n",
    "    b1 = np.zeros((2, 1))\n",
    "    W2 = np.random.uniform(-1, 1, (1, 2))\n",
    "    b2 = np.zeros((1, 1))\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        A2, cache = forward_propagation(X, W1, b1, W2, b2)\n",
    "        loss = compute_loss(A2, Y)   \n",
    "        grads = backward_propagation(X, Y, cache, W2)   \n",
    "        W1, b1, W2, b2 = update_parameters(W1, b1, W2, b2, grads, learning_rate)\n",
    "        \n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(f\"Iteration {i}, loss: {loss:.6f}\")\n",
    "    \n",
    "    parameters = {\"W1\": W1, \"b1\": b1, \"W2\": W2, \"b2\": b2}\n",
    "    return parameters"
   ],
   "id": "203d194e79386592",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-23T14:30:08.733323Z",
     "start_time": "2025-02-23T14:30:07.984466Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0, loss: 0.770870\n",
      "Iteration 1000, loss: 0.514218\n",
      "Iteration 2000, loss: 0.043021\n",
      "Iteration 3000, loss: 0.019736\n",
      "Iteration 4000, loss: 0.012691\n",
      "Iteration 5000, loss: 0.009327\n",
      "Iteration 6000, loss: 0.007364\n",
      "Iteration 7000, loss: 0.006079\n",
      "Iteration 8000, loss: 0.005174\n",
      "Iteration 9000, loss: 0.004503\n",
      "Predictions: [[0 1 1 0]]\n",
      "Accuracy: 100.00%\n"
     ]
    }
   ],
   "execution_count": 2,
   "source": [
    "X = np.array([[0, 0, 1, 1],\n",
    "              [0, 1, 0, 1]])\n",
    "Y = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "parameters = model(X, Y, num_iterations=10000, learning_rate=0.5)\n",
    "\n",
    "A2, cache = forward_propagation(X, parameters[\"W1\"], parameters[\"b1\"], \n",
    "                                  parameters[\"W2\"], parameters[\"b2\"])\n",
    "predictions = (A2 >= 0.5).astype(int)\n",
    "accuracy = np.mean(predictions == Y) * 100\n",
    "print(\"Predictions:\", predictions)\n",
    "print(\"Accuracy: {:.2f}%\".format(accuracy))\n"
   ],
   "id": "19083d2ac22695d0"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
