{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contrastive Learning CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import datetime\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import AutoImageProcessor, TimesformerModel, TimesformerConfig\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "from torch.utils.data._utils.collate import default_collate\n",
    "import timesformer.models.optimizer as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from timesformer.datasets.rtmri75s import Rtmri75s\n",
    "from timesformer.utils.parser import load_config, parse_args\n",
    "# from timesformer.datasets.loader import detection_collate\n",
    "from torch.utils.data._utils.collate import default_collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIP(nn.Module):\n",
    "    def __init__(self,\n",
    "                 device = 'cuda:0'):\n",
    "        super().__init__()\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "        self.device = device\n",
    "    def build_attention_mask(self):\n",
    "        # lazily create causal attention mask, with full attention between the vision tokens\n",
    "        # pytorch uses additive attention mask; fill with -inf\n",
    "        mask = torch.empty(self.context_length, self.context_length)\n",
    "        mask.fill_(float(\"-inf\"))\n",
    "        mask.triu_(1)  # zero out the lower diagonal\n",
    "        return mask\n",
    "\n",
    "    @property\n",
    "    def dtype(self):\n",
    "        return self.visual.conv1.weight.dtype\n",
    "\n",
    "    def encode_image(self, video):\n",
    "        cfg = TimesformerConfig(hidden_size=1024, num_attention_heads = 16)\n",
    "        video_model = TimesformerModel(cfg).to(self.device)\n",
    "        # video_model = TimesformerModel.from_pretrained(\"facebook/timesformer-hr-finetuned-k600\")\n",
    "        return video_model(video)\n",
    "\n",
    "    def forward(self, video, audio_features):\n",
    "        video_features = torch.mean(self.encode_image(video).last_hidden_state, dim=1)\n",
    "        # print(video_features.shape)\n",
    "        # audio_features = self.encode_text(text)\n",
    "\n",
    "        # normalized features\n",
    "        # print(video_features.shape)\n",
    "        video_features = torch.nn.functional.normalize(video_features, p=2, dim=1)#video_features / video_features.norm(dim=1, keepdim=True) \n",
    "        audio_features = torch.nn.functional.normalize(audio_features, p=2, dim=1)#audio_features / audio_features.norm(dim=1, keepdim=True)\n",
    "\n",
    "        # cosine similarity as logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * video_features @ audio_features.t()\n",
    "        logits_per_text = logits_per_image.t()\n",
    "\n",
    "        # shape = [global_batch_size, global_batch_size]\n",
    "        return logits_per_image, logits_per_text\n",
    "\n",
    "class ClipLoss(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            local_loss=False,\n",
    "            gather_with_grad=False,\n",
    "            cache_labels=False,\n",
    "            rank=0,\n",
    "            world_size=1,\n",
    "            use_horovod=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.local_loss = local_loss\n",
    "        self.gather_with_grad = gather_with_grad\n",
    "        self.cache_labels = cache_labels\n",
    "        self.rank = rank\n",
    "        self.world_size = world_size\n",
    "        self.use_horovod = use_horovod\n",
    "\n",
    "        # cache state\n",
    "        self.prev_num_logits = 0\n",
    "        self.labels = {}\n",
    "\n",
    "    def get_ground_truth(self, device, num_logits) -> torch.Tensor:\n",
    "        # calculated ground-truth and cache if enabled\n",
    "        if self.prev_num_logits != num_logits or device not in self.labels:\n",
    "            labels = torch.arange(num_logits, device=device, dtype=torch.long)\n",
    "            if self.world_size > 1 and self.local_loss:\n",
    "                labels = labels + num_logits * self.rank\n",
    "            if self.cache_labels:\n",
    "                self.labels[device] = labels\n",
    "                self.prev_num_logits = num_logits\n",
    "        else:\n",
    "            labels = self.labels[device]\n",
    "        return labels\n",
    "\n",
    "    def forward(self, logits_per_image, logits_per_text, output_dict=False):\n",
    "        device = logits_per_image.device\n",
    "        # logits_per_image, logits_per_text = self.get_logits(image_features, text_features, logit_scale)\n",
    "\n",
    "        labels = self.get_ground_truth(device, logits_per_image.shape[0])\n",
    "\n",
    "        total_loss = (\n",
    "            F.cross_entropy(logits_per_image, labels) +\n",
    "            F.cross_entropy(logits_per_text, labels)\n",
    "        ) / 2\n",
    "\n",
    "        return {\"contrastive_loss\": total_loss} if output_dict else total_loss\n",
    "    \n",
    "# model = CLIP()\n",
    "# loss_fun = ClipLoss()\n",
    "# videos, audios =dataset[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logits_per_image, logits_per_text = model(rearrange(videos[1].unsqueeze(0), 'b c t h w -> b t c h w'), audios[0].unsqueeze(0))\n",
    "# loss = loss_fun(logits_per_image, logits_per_text)\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_model = TimesformerModel.from_pretrained(\"facebook/timesformer-base-finetuned-k600\")\n",
    "# video_model(rearrange(videos[1].unsqueeze(0), 'b c t h w -> b t c h w')).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import timesformer.models.optimizer as optim\n",
    "# optimizer = optim.construct_optimizer(model, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(\n",
    "    train_loader, model, optimizer, cur_epoch, cfg\n",
    "):\n",
    "    total_loss = 0\n",
    "    loss_fun = ClipLoss()\n",
    "    cur_iter = 0\n",
    "    for videos, audios, _, _ in tqdm(train_loader):\n",
    "        data_size = len(train_loader)\n",
    "        # print(data_size)\n",
    "        # model = model.train()\n",
    "        # model = model.cuda()\n",
    "\n",
    "        videos = rearrange(videos[1], 'b c t h w -> b t c h w').cuda(non_blocking=True)\n",
    "        audios_embs = audios.cuda(non_blocking=True)\n",
    "\n",
    "        lr = optim.get_epoch_lr(cur_epoch + float(cur_iter) / data_size, cfg)\n",
    "\n",
    "        logits_per_image, logits_per_text = model(videos, audios_embs)\n",
    "\n",
    "        loss = loss_fun(logits_per_image, logits_per_text)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        cur_iter += 1\n",
    "        \n",
    "    return total_loss\n",
    "\n",
    "def train(cfg):\n",
    "    batch_size = int(cfg.TRAIN.BATCH_SIZE / max(1, cfg.NUM_GPUS))\n",
    "    # Construct the dataset\n",
    "    dataset = Rtmri75s(cfg, \"train\")\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=cfg.DATA_LOADER.NUM_WORKERS,\n",
    "        pin_memory=cfg.DATA_LOADER.PIN_MEMORY,\n",
    "        collate_fn=default_collate,\n",
    "    )\n",
    "\n",
    "    model = CLIP()\n",
    "    # loss_fun = ClipLoss()\n",
    "    device = 'cuda:0'\n",
    "    optimizer = optim.construct_optimizer(model, cfg)\n",
    "    \n",
    "    model = torch.load(\"/data2/hongn/TimeSformer/lowest_lost_2024-12-13.pth\")\n",
    "    model.train()\n",
    "    model = model.to(device)\n",
    "\n",
    "    min_loss = 100000\n",
    "\n",
    "    for e in range(1000):\n",
    "        total_loss = train_epoch(train_loader, model, optimizer, cur_epoch = e, cfg=cfg)\n",
    "        print(f\"EPOCH {e} with total loss {total_loss}\")\n",
    "        if(total_loss < min_loss):\n",
    "            min_loss = total_loss\n",
    "            today = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "            torch.save(model, f'lowest_lost_{today}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [03:58<00:00,  3.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 0 with total loss 257.564825296402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 30/75 [01:37<02:24,  3.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to meta load audio idx 1490 from /data1/span_data/rtmri75s/sub050/2drt/video/sub050_2drt_01_vcv1_r1_video.mp4; trial 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [04:01<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1 with total loss 257.56669449806213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 37/75 [02:00<02:01,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to meta load audio idx 1709 from /data1/span_data/rtmri75s/sub068/2drt/video/sub068_2drt_09_northwind1_r2_video.mp4; trial 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 57/75 [03:04<00:57,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to meta load audio idx 2358 from /data1/span_data/rtmri75s/sub047/2drt/video/sub047_2drt_15_picture4_video.mp4; trial 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [04:01<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 2 with total loss 257.56507873535156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [04:01<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 3 with total loss 257.5650157928467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to meta load audio idx 24 from /data1/span_data/rtmri75s/sub007/2drt/video/sub007_2drt_07_grandfather1_r1_video.mp4; trial 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [04:00<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 4 with total loss 257.56096029281616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [04:01<00:00,  3.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 5 with total loss 257.56329321861267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/75 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to meta load audio idx 19 from /data1/span_data/rtmri75s/sub007/2drt/video/sub007_2drt_14_picture3_video.mp4; trial 0\n",
      "Failed to meta load audio idx 94 from /data1/span_data/rtmri75s/sub063/2drt/video/sub063_2drt_03_vcv3_r1_video.mp4; trial 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [04:01<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 6 with total loss 257.5659623146057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [04:01<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 7 with total loss 257.56448125839233\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [04:01<00:00,  3.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 8 with total loss 257.5644246339798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2/75 [00:08<04:47,  3.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to meta load audio idx 599 from /data1/span_data/rtmri75s/sub062/2drt/video/sub062_2drt_03_vcv3_r1_video.mp4; trial 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 16/75 [00:55<03:25,  3.49s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cfg \u001b[38;5;241m=\u001b[39m load_config(cfg_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/data2/hongn/TimeSformer/configs/Rtmri75s/simple_cfg.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[25], line 56\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(cfg)\u001b[0m\n\u001b[1;32m     53\u001b[0m min_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100000\u001b[39m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[0;32m---> 56\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcur_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEPOCH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with total loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m(total_loss \u001b[38;5;241m<\u001b[39m min_loss):\n",
      "Cell \u001b[0;32mIn[25], line 23\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(train_loader, model, optimizer, cur_epoch, cfg)\u001b[0m\n\u001b[1;32m     20\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fun(logits_per_image, logits_per_text)\n\u001b[1;32m     22\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 23\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     24\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     26\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cfg = load_config(cfg_file = \"/data2/hongn/TimeSformer/configs/Rtmri75s/simple_cfg.yaml\")\n",
    "train(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata_size\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_size' is not defined"
     ]
    }
   ],
   "source": [
    "data_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1569, 1024])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TimesformerConfig, TimesformerModel\n",
    "cfg = TimesformerConfig(hidden_size=1024, num_attention_heads = 16)\n",
    "video_model = TimesformerModel(cfg)\n",
    "video_model(rearrange(videos[1].unsqueeze(0), 'b c t h w -> b t c h w')).last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data1/span_data/rtmri75s/sub007/2drt/video/sub007_2drt_13_picture2_video.mp4\n",
      "video_size 2738 start_idx 305.5214309103746 end_idx 331.17034256210314\n",
      "video_start_pts 238612 video_end_pts 258644 frames_length 2738 duration 2138378 start_idx 305.5214309103746 end_idx 331.17034256210314\n",
      "video_size 27 start_idx 0.0 end_idx 25.648911651728554\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1024])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aud = dataset[2][1]\n",
    "aud.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating validation split: 0 examples [00:00, ? examples/s]\n"
     ]
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/features/audio.py:88\u001b[0m, in \u001b[0;36mAudio.encode_example\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msoundfile\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msf\u001b[39;00m  \u001b[38;5;66;03m# soundfile is a dependency of librosa, needed to decode audio files.\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'soundfile'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/builder.py:1624\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1615\u001b[0m     writer \u001b[38;5;241m=\u001b[39m writer_class(\n\u001b[1;32m   1616\u001b[0m         features\u001b[38;5;241m=\u001b[39mwriter\u001b[38;5;241m.\u001b[39m_features,\n\u001b[1;32m   1617\u001b[0m         path\u001b[38;5;241m=\u001b[39mfpath\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSSSSS\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshard_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJJJJJ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjob_id\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m05d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1622\u001b[0m         embed_local_files\u001b[38;5;241m=\u001b[39membed_local_files,\n\u001b[1;32m   1623\u001b[0m     )\n\u001b[0;32m-> 1624\u001b[0m example \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mfeatures \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m record\n\u001b[1;32m   1625\u001b[0m writer\u001b[38;5;241m.\u001b[39mwrite(example, key)\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/features/features.py:1994\u001b[0m, in \u001b[0;36mFeatures.encode_example\u001b[0;34m(self, example)\u001b[0m\n\u001b[1;32m   1993\u001b[0m example \u001b[38;5;241m=\u001b[39m cast_to_python_objects(example)\n\u001b[0;32m-> 1994\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/features/features.py:1282\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot None but expected a dictionary instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1282\u001b[0m         {k: encode_nested_example(schema[k], obj\u001b[38;5;241m.\u001b[39mget(k), level\u001b[38;5;241m=\u001b[39mlevel \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m schema}\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m     )\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/features/features.py:1282\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot None but expected a dictionary instead\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m-> 1282\u001b[0m         {k: \u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m[\u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m schema}\n\u001b[1;32m   1283\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1284\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1285\u001b[0m     )\n\u001b[1;32m   1287\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/features/features.py:1352\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1351\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(schema, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencode_example\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1352\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1353\u001b[0m \u001b[38;5;66;03m# Other object should be directly convertible to a native Arrow type (like Translation and Translation)\u001b[39;00m\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/features/audio.py:90\u001b[0m, in \u001b[0;36mAudio.encode_example\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m---> 90\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTo support encoding audio data, please install \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoundfile\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m):\n",
      "\u001b[0;31mImportError\u001b[0m: To support encoding audio data, please install 'soundfile'.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Wav2Vec2ForCTC\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfacebook/wav2vec2-lv-60-espeak-cv-ft\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# load dummy dataset and read soundfiles\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m ds \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpatrickvonplaten/librispeech_asr_dummy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalidation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# tokenize\u001b[39;00m\n\u001b[1;32m     13\u001b[0m input_values \u001b[38;5;241m=\u001b[39m processor(ds[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m], return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39minput_values\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/load.py:2151\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2150\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2151\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2152\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2153\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2154\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2160\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2161\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2162\u001b[0m )\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 924\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/builder.py:1648\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001b[0m\n\u001b[1;32m   1647\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_download_and_prepare\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager, verification_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mprepare_splits_kwargs):\n\u001b[0;32m-> 1648\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1651\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_duplicate_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mBASIC_CHECKS\u001b[49m\n\u001b[1;32m   1652\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mVerificationMode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mALL_CHECKS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1653\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_splits_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1654\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/builder.py:1000\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1007\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/builder.py:1486\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, check_duplicate_keys, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1484\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1485\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1486\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1487\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1488\u001b[0m     ):\n\u001b[1;32m   1489\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1490\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/datasets/builder.py:1643\u001b[0m, in \u001b[0;36mGeneratorBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, split_info, check_duplicate_keys, job_id)\u001b[0m\n\u001b[1;32m   1641\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, SchemaInferenceError) \u001b[38;5;129;01mand\u001b[39;00m e\u001b[38;5;241m.\u001b[39m__context__ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1642\u001b[0m         e \u001b[38;5;241m=\u001b[39m e\u001b[38;5;241m.\u001b[39m__context__\n\u001b[0;32m-> 1643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while generating the dataset\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer\u001b[38;5;241m.\u001b[39m_features, num_shards, shard_lengths)\n",
      "\u001b[0;31mDatasetGenerationError\u001b[0m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "    \n",
    "# load dummy dataset and read soundfiles\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# tokenize\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.9145, -1.9044, -1.8860,  ..., -0.8597, -0.9200, -1.0656],\n",
       "         [-1.9020, -1.8946, -1.8788,  ..., -0.8741, -0.9341, -1.0755],\n",
       "         [-1.8912, -1.8882, -1.8748,  ..., -0.9746, -1.0234, -1.1309],\n",
       "         ...,\n",
       "         [-1.7798, -1.7348, -1.6976,  ..., -1.3505, -1.3566, -1.3685],\n",
       "         [-1.7778, -1.7343, -1.7171,  ..., -1.3809, -1.3952, -1.4233],\n",
       "         [-1.7759, -1.7339, -1.7366,  ..., -1.4113, -1.4337, -1.4780]],\n",
       "\n",
       "        [[-1.8797, -1.8695, -1.8511,  ..., -0.8249, -0.8852, -1.0308],\n",
       "         [-1.8671, -1.8597, -1.8440,  ..., -0.8392, -0.8993, -1.0406],\n",
       "         [-1.8563, -1.8533, -1.8399,  ..., -0.9398, -0.9885, -1.0960],\n",
       "         ...,\n",
       "         [-1.7449, -1.6999, -1.6628,  ..., -1.3157, -1.3217, -1.3336],\n",
       "         [-1.7430, -1.6995, -1.6822,  ..., -1.3461, -1.3603, -1.3884],\n",
       "         [-1.7411, -1.6990, -1.7017,  ..., -1.3765, -1.3989, -1.4432]],\n",
       "\n",
       "        [[-1.9319, -1.9218, -1.9034,  ..., -0.8771, -0.9375, -1.0831],\n",
       "         [-1.9194, -1.9120, -1.8963,  ..., -0.8915, -0.9516, -1.0929],\n",
       "         [-1.9086, -1.9056, -1.8922,  ..., -0.9921, -1.0408, -1.1483],\n",
       "         ...,\n",
       "         [-1.7972, -1.7522, -1.7150,  ..., -1.3680, -1.3740, -1.3859],\n",
       "         [-1.7953, -1.7518, -1.7345,  ..., -1.3984, -1.4126, -1.4407],\n",
       "         [-1.7933, -1.7513, -1.7540,  ..., -1.4288, -1.4512, -1.4955]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos[1][:,0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TimesformerModel(\n",
       "  (embeddings): TimesformerEmbeddings(\n",
       "    (patch_embeddings): TimesformerPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "    (time_drop): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): TimesformerEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (1): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (2): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (3): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (4): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (5): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (6): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (7): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (8): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (9): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (10): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "      (11): TimesformerLayer(\n",
       "        (drop_path): Identity()\n",
       "        (attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): TimesformerIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): TimesformerOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (temporal_attention): TimeSformerAttention(\n",
       "          (attention): TimesformerSelfAttention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (output): TimesformerSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Sample of Wav2vec Phoneme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function. Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "\n",
    "# load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "    \n",
    "# load dummy dataset and read soundfiles\n",
    "ds = load_dataset(\"patrickvonplaten/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n",
    "\n",
    "# tokenize\n",
    "input_values = processor(ds[0][\"audio\"][\"array\"], return_tensors=\"pt\").input_values\n",
    "\n",
    "# retrieve logits\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "in_path = \"/data1/span_data/rtmri75s/sub001/2drt/audio/\"\n",
    "name = \"sub001_2drt_19_topic3_audio\"\n",
    "input_audio, sample_rate = librosa.load(f\"{in_path}/{name}.wav\",  sr=16000)\n",
    "\n",
    "input_values = processor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate).input_values\n",
    "\n",
    "# retrieve logits\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "\n",
    "# take argmax and decode\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.5315111e-04, -1.8992375e-03, -4.2232666e-03, ...,\n",
       "         2.5964671e-01,  4.1573516e-01,  3.2189554e-01], dtype=float32),\n",
       " 16000)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.load(\"/data1/span_data/rtmri75s/sub007/2drt/audio/sub007_2drt_12_picture1_audio.wav\", sr=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CausalLMOutput' object has no attribute 'last_hidden_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlast_hidden_state\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CausalLMOutput' object has no attribute 'last_hidden_state'"
     ]
    }
   ],
   "source": [
    "o.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  5,  0,  5,  0,  0, 33,  0,  0,  0,  0, 13,  0,  0,  0,\n",
       "         0,  0, 27, 27,  0, 33,  0,  0,  0,  0,  0,  0,  0, 23,  0,  0,  0,  0,\n",
       "         6,  0,  0,  0, 44,  0,  0,  0,  0,  0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_ids[0][1100:1150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1682 * 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide wav clips into 200ms video-audio segments? Or should I random sample video-audiop pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob.glob(\"/data1/span_data/rtmri75s/sub0[6-8]*\")\n",
    "video_paths = []\n",
    "audio_paths = []\n",
    "for i in folders:\n",
    "    sub_video_paths = glob.glob(i + \"/2drt/video/*\")\n",
    "    video_paths = video_paths + sub_video_paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_paths = []\n",
    "for path in video_paths:\n",
    "    # name = \"/data1/hongn/wav2vec_75speaker/\" + path.split('/')[-1].split('.')[0] + '.pt'\n",
    "    name = path.split('video')[0] + 'audio' + path.split('video')[1] + 'audio.wav'\n",
    "    audio_paths.append(name)\n",
    "    try:\n",
    "        # name = path.split('video')[0] + 'audio' + path.split('video')[1] + 'audio.wav'\n",
    "        assert os.path.exists(name), f\"Path not exist {name}\"\n",
    "    except Exception as e:\n",
    "        print(f\"WARNING: {e}\")\n",
    "        os.system(f\"ffmpeg -i {path} {name}\")\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_08_grandfather2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub063/2drt/audio/sub063_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_08_grandfather2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub073/2drt/audio/sub073_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_08_grandfather2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub075/2drt/audio/sub075_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub072/2drt/audio/sub072_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_08_grandfather2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub062/2drt/audio/sub062_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_08_grandfather2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub066/2drt/audio/sub066_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_08_grandfather2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub069/2drt/audio/sub069_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_08_grandfather2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub061/2drt/audio/sub061_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub067/2drt/audio/sub067_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_08_grandfather2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub060/2drt/audio/sub060_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_08_grandfather2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub068/2drt/audio/sub068_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub071/2drt/audio/sub071_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_08_grandfather2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub064/2drt/audio/sub064_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_08_grandfather2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub065/2drt/audio/sub065_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub070/2drt/audio/sub070_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_20_topic4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_09_northwind1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_02_vcv2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_11_postures_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_05_shibboleth_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_18_topic2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_05_shibboleth_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_10_northwind2_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_07_grandfather1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_03_vcv3_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_13_picture2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_04_bvt_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_04_bvt_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_16_picture5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_14_picture3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_21_topic5_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_08_grandfather2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_09_northwind1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_10_northwind2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_12_picture1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_02_vcv2_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_19_topic3_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_01_vcv1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_17_topic1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_01_vcv1_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_06_rainbow_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_03_vcv3_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_11_postures_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_15_picture4_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_07_grandfather1_r2_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_06_rainbow_r1_audio.wav',\n",
       " '/data1/span_data/rtmri75s/sub074/2drt/audio/sub074_2drt_08_grandfather2_r2_audio.wav']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_paths</th>\n",
       "      <th>audio_paths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/data1/span_data/rtmri75s/sub063/2drt/video/su...</td>\n",
       "      <td>/data1/span_data/rtmri75s/sub063/2drt/audio/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/data1/span_data/rtmri75s/sub063/2drt/video/su...</td>\n",
       "      <td>/data1/span_data/rtmri75s/sub063/2drt/audio/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/data1/span_data/rtmri75s/sub063/2drt/video/su...</td>\n",
       "      <td>/data1/span_data/rtmri75s/sub063/2drt/audio/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/data1/span_data/rtmri75s/sub063/2drt/video/su...</td>\n",
       "      <td>/data1/span_data/rtmri75s/sub063/2drt/audio/su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>/data1/span_data/rtmri75s/sub063/2drt/video/su...</td>\n",
       "      <td>/data1/span_data/rtmri75s/sub063/2drt/audio/su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         video_paths  \\\n",
       "0  /data1/span_data/rtmri75s/sub063/2drt/video/su...   \n",
       "1  /data1/span_data/rtmri75s/sub063/2drt/video/su...   \n",
       "2  /data1/span_data/rtmri75s/sub063/2drt/video/su...   \n",
       "3  /data1/span_data/rtmri75s/sub063/2drt/video/su...   \n",
       "4  /data1/span_data/rtmri75s/sub063/2drt/video/su...   \n",
       "\n",
       "                                         audio_paths  \n",
       "0  /data1/span_data/rtmri75s/sub063/2drt/audio/su...  \n",
       "1  /data1/span_data/rtmri75s/sub063/2drt/audio/su...  \n",
       "2  /data1/span_data/rtmri75s/sub063/2drt/audio/su...  \n",
       "3  /data1/span_data/rtmri75s/sub063/2drt/audio/su...  \n",
       "4  /data1/span_data/rtmri75s/sub063/2drt/audio/su...  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_audio_paths = pd.DataFrame(\n",
    "    {'video_paths': video_paths,\n",
    "     'audio_paths': audio_paths,\n",
    "    })\n",
    "video_audio_paths.head()\n",
    "\n",
    "video_audio_paths.to_csv('/data1/hongn/val.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_audio_paths.to_csv('/data1/hongn/val.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.read_csv(\"/data1/hongn/train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split videos into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def video_len_with_opencv(filename):\n",
    "    video = cv2.VideoCapture(filename)\n",
    "\n",
    "    fps = video.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count = video.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "    duration = frame_count/fps\n",
    "    return duration, frame_count, fps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42.82051045510455, 3566.0, 83.27784891165173)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_len_with_opencv(video_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data1/span_data/rtmri75s/sub007/2drt/video/sub007_2drt_01_vcv1_r2_video.mp4'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 6.1.1 Copyright (c) 2000-2023 the FFmpeg developers\n",
      "  built with gcc 12.3.0 (conda-forge gcc 12.3.0-7)\n",
      "  configuration: --prefix=/data2/hongn/miniconda3/envs/sapiens_lite --cc=/home/conda/feedstock_root/build_artifacts/ffmpeg_1716145014501/_build_env/bin/x86_64-conda-linux-gnu-cc --cxx=/home/conda/feedstock_root/build_artifacts/ffmpeg_1716145014501/_build_env/bin/x86_64-conda-linux-gnu-c++ --nm=/home/conda/feedstock_root/build_artifacts/ffmpeg_1716145014501/_build_env/bin/x86_64-conda-linux-gnu-nm --ar=/home/conda/feedstock_root/build_artifacts/ffmpeg_1716145014501/_build_env/bin/x86_64-conda-linux-gnu-ar --disable-doc --disable-openssl --enable-demuxer=dash --enable-hardcoded-tables --enable-libfreetype --enable-libharfbuzz --enable-libfontconfig --enable-libopenh264 --enable-libdav1d --enable-gnutls --enable-libmp3lame --enable-libvpx --enable-libass --enable-pthreads --enable-vaapi --enable-libopenvino --enable-gpl --enable-libx264 --enable-libx265 --enable-libaom --enable-libsvtav1 --enable-libxml2 --enable-pic --enable-shared --disable-static --enable-version3 --enable-zlib --enable-libopus --pkg-config=/home/conda/feedstock_root/build_artifacts/ffmpeg_1716145014501/_build_env/bin/pkg-config\n",
      "  libavutil      58. 29.100 / 58. 29.100\n",
      "  libavcodec     60. 31.102 / 60. 31.102\n",
      "  libavformat    60. 16.100 / 60. 16.100\n",
      "  libavdevice    60.  3.100 / 60.  3.100\n",
      "  libavfilter     9. 12.100 /  9. 12.100\n",
      "  libswscale      7.  5.100 /  7.  5.100\n",
      "  libswresample   4. 12.100 /  4. 12.100\n",
      "  libpostproc    57.  3.100 / 57.  3.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from '/data1/span_data/rtmri75s/sub007/2drt/video/sub007_2drt_01_vcv1_r2_video.mp4':\n",
      "  Metadata:\n",
      "    major_brand     : isom\n",
      "    minor_version   : 512\n",
      "    compatible_brands: isomiso2mp41\n",
      "    encoder         : Lavf58.46.101\n",
      "  Duration: 00:00:43.01, start: 0.000000, bitrate: 314 kb/s\n",
      "  Stream #0:0[0x1](und): Video: mpeg4 (Simple Profile) (mp4v / 0x7634706D), yuv420p, 84x84 [SAR 1:1 DAR 1:1], 240 kb/s, 83.28 fps, 83.28 tbr, 65040 tbn (default)\n",
      "    Metadata:\n",
      "      handler_name    : VideoHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 22050 Hz, mono, fltp, 69 kb/s (default)\n",
      "    Metadata:\n",
      "      handler_name    : SoundHandler\n",
      "      vendor_id       : [0][0][0][0]\n",
      "File '/data1/hongn/rtmri75s_processed/video/sub007_2drt_01_vcv1_r2_video_chunk_0.0.mp4' already exists. Overwrite? [y/N] "
     ]
    }
   ],
   "source": [
    "for vidpath in video_paths:\n",
    "    i = 0\n",
    "    duration, frame_count, fps = video_len_with_opencv(vidpath)\n",
    "    while i < duration:\n",
    "        chunk_name = '/data1/hongn/rtmri75s_processed/video/' + video_paths[0].split('/')[-1].split('.')[0] + '_chunk_{:.1f}.mp4'.format(i)\n",
    "        os.system(f\"ffmpeg -i {vidpath} -ss {i} -t {i + 0.2} {chunk_name}\")\n",
    "        i += 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data1/hongn/rtmri75s_processed/video/sub007_2drt_01_vcv1_r2_video_chunk_0.0.mp4'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "'/data1/hongn/rtmri75s_processed/video/' + video_paths[0].split('/')[-1].split('.')[0] + '_chunk_{:.1f}.mp4'.format(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data1/span_data/rtmri75s/sub007/2drt/audio/sub007_2drt_01_vcv1_r2_audio.wav'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"ffmpeg -i source-file.mp4 -ss 0 -t 1 first-10-min.m4v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.concatenate(([0], [0], [0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pre-Extract Wav2Vec Phoneme logits to .pt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "from datasets import load_dataset\n",
    "import torch\n",
    "import librosa\n",
    "\n",
    "# load model and processor\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-lv-60-espeak-cv-ft\")\n",
    "\n",
    "\n",
    "\n",
    "for audio_path in audio_paths:\n",
    "    input_audio, sample_rate = librosa.load(audio_path,  sr=16000)\n",
    "\n",
    "    input_values = processor(input_audio, return_tensors=\"pt\", sampling_rate=sample_rate).input_values\n",
    "\n",
    "    # retrieve logits\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    # take argmax and decode\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)\n",
    "\n",
    "    torch.save(predicted_ids, '/data1/hongn/rtmri75s_processed/audio/' + audio_path.split('.')[0].split('/')[-1] + '.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data1/hongn/rtmri75s_processed/audio/sub007_2drt_01_vcv1_r2_audio.pt'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'/data1/hongn/rtmri75s_processed/audio/' + audio_paths[0].split('.')[0].split('/')[-1] + '.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(predicted_ids, '/data1/hongn/rtmri75s_processed/audio/' + audio_paths[0].split('.')[0].split('/')[-1] + '.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train.py val.py for rtmridata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/data1/hongn/rtmri75s_processed/video/sub036_2drt_19_topic3_video_chunk_23.2.mp4'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import glob\n",
    "import torch\n",
    "\n",
    "chunks_paths = glob.glob('/data1/hongn/rtmri75s_processed/video/sub0[0-5]*')\n",
    "chunks_paths[0]\n",
    "# audio_emb_paths = glob.glob('/data1/hongn/rtmri75s_processed/audio/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp = chunks_paths[430].split('/')[-1].split('_chunk_')\n",
    "# start_idx = int(float(temp[-1].split('.mp4')[0])/0.02)\n",
    "# audio_path = '/data1/hongn/rtmri75s_processed/audio/' + temp[0].split('_video')[0] + '_audio.pt'\n",
    "# audio_logits = torch.load(audio_path)\n",
    "# audio_logits[0,start_idx:start_idx+10].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start_idx = 0\n",
    "# logit_not0 = [x for x in audio_logits[0,start_idx:start_idx+10].numpy() if x != 0]\n",
    "# if logit_not0 == []:\n",
    "#     logit_not0 = [0]\n",
    "# logit_not0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_chunks_processed = []\n",
    "audio_labels_processed = []\n",
    "\n",
    "for path in chunks_paths:\n",
    "    temp = path.split('/')[-1].split('_chunk_')\n",
    "    start_idx = int(float(temp[-1].split('.mp4')[0])/0.02)\n",
    "    audio_path = '/data1/hongn/rtmri75s_processed/audio/' + temp[0].split('_video')[0] + '_audio.pt'\n",
    "    audio_logits = torch.load(audio_path)\n",
    "\n",
    "    logit_not0 = [x for x in audio_logits[0,start_idx:start_idx+10].numpy() if x != 0]\n",
    "    if logit_not0 == []:\n",
    "        logit_not0 = [0]\n",
    "    for logit in logit_not0:\n",
    "        video_chunks_processed.append(path)\n",
    "        audio_labels_processed.append(logit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "441388"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(video_chunks_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_audio_paths = pd.DataFrame(\n",
    "    {'video_paths': video_chunks_processed,\n",
    "     'audio_paths': audio_labels_processed,\n",
    "    })\n",
    "\n",
    "video_audio_paths.to_csv('/data1/hongn/rtmri75s_processed/train.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_logits[0,start_idx:start_idx+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data2/hongn/miniconda3/envs/sapiens_lite/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: libtorch_cuda_cu.so: cannot open shared object file: No such file or directory\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n",
      "/data2/hongn/TimeSformer/timesformer/models/moose.py:149: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  motion_model.load_state_dict(torch.load(args.model))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Since no pretrained weights have been provided, we load the reference pretrained DINO weights.\n"
     ]
    }
   ],
   "source": [
    "from timesformer.models.vit import MOOSE\n",
    "import argparse\n",
    "from timesformer.models.moose import MOOSE_Encoder, CustomAttentionWithResidual\n",
    "# from \n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', help=\"restore checkpoint\")\n",
    "parser.add_argument('--path', help=\"dataset for evaluation\")\n",
    "parser.add_argument('--small', action='store_true', help='use small model')\n",
    "parser.add_argument('--mixed_precision', action='store_true', help='use mixed precision')\n",
    "parser.add_argument('--alternate_corr', action='store_true', help='use efficent correlation implementation')\n",
    "\n",
    "raft_args = parser.parse_args(['--model', '/data2/hongn/RAFT/models/raft-things.pth', \n",
    "                        '--path', '/data2/hongn/RAFT/demo-frames/care'])\n",
    "moose = MOOSE(raft_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MOOSE(\n",
       "  (crossatt): CustomAttentionWithResidual(\n",
       "    (attention): CustomAttention(\n",
       "      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (fc_out): Linear(in_features=768, out_features=768, bias=True)\n",
       "    )\n",
       "    (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (moose_encoder): MOOSE_Encoder(\n",
       "    (motion_model): RAFT(\n",
       "      (fnet): BasicEncoder(\n",
       "        (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (layer1): Sequential(\n",
       "          (0): ResidualBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          )\n",
       "          (1): ResidualBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (norm2): InstanceNorm2d(64, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): ResidualBlock(\n",
       "            (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (norm2): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (norm3): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(2, 2))\n",
       "              (1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualBlock(\n",
       "            (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (norm2): InstanceNorm2d(96, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): ResidualBlock(\n",
       "            (conv1): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (norm3): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "              (1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualBlock(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "            (norm2): InstanceNorm2d(128, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)\n",
       "          )\n",
       "        )\n",
       "        (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (cnet): BasicEncoder(\n",
       "        (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3))\n",
       "        (relu1): ReLU(inplace=True)\n",
       "        (layer1): Sequential(\n",
       "          (0): ResidualBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "          (1): ResidualBlock(\n",
       "            (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (layer2): Sequential(\n",
       "          (0): ResidualBlock(\n",
       "            (conv1): Conv2d(64, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm3): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(64, 96, kernel_size=(1, 1), stride=(2, 2))\n",
       "              (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualBlock(\n",
       "            (conv1): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (layer3): Sequential(\n",
       "          (0): ResidualBlock(\n",
       "            (conv1): Conv2d(96, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (downsample): Sequential(\n",
       "              (0): Conv2d(96, 128, kernel_size=(1, 1), stride=(2, 2))\n",
       "              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualBlock(\n",
       "            (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "            (relu): ReLU(inplace=True)\n",
       "            (norm1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "            (norm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "          )\n",
       "        )\n",
       "        (conv2): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (update_block): BasicUpdateBlock(\n",
       "        (encoder): BasicMotionEncoder(\n",
       "          (convc1): Conv2d(324, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "          (convc2): Conv2d(256, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (convf1): Conv2d(2, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
       "          (convf2): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv): Conv2d(256, 126, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "        (gru): SepConvGRU(\n",
       "          (convz1): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
       "          (convr1): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
       "          (convq1): Conv2d(384, 128, kernel_size=(1, 5), stride=(1, 1), padding=(0, 2))\n",
       "          (convz2): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
       "          (convr2): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
       "          (convq2): Conv2d(384, 128, kernel_size=(5, 1), stride=(1, 1), padding=(2, 0))\n",
       "        )\n",
       "        (flow_head): FlowHead(\n",
       "          (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (conv2): Conv2d(256, 2, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (mask): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "          (2): Conv2d(256, 576, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (visual_model): VisionTransformer(\n",
       "      (patch_embed): PatchEmbed(\n",
       "        (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "      (blocks): ModuleList(\n",
       "        (0-11): 12 x Block(\n",
       "          (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (attn): Attention(\n",
       "            (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "            (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "            (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (drop_path): Identity()\n",
       "          (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "          (mlp): Mlp(\n",
       "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (act): GELU(approximate='none')\n",
       "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (drop): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (head): Identity()\n",
       "    )\n",
       "  )\n",
       "  (patch_embed): MotionPatchEmbed(\n",
       "    (proj): Conv2d(2, 768, kernel_size=(2, 2), stride=(2, 2))\n",
       "  )\n",
       "  (head): Linear(in_features=768, out_features=400, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "moose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crossatt.attention.query.weight\n",
      "crossatt.attention.query.bias\n",
      "crossatt.attention.key.weight\n",
      "crossatt.attention.key.bias\n",
      "crossatt.attention.value.weight\n",
      "crossatt.attention.value.bias\n",
      "crossatt.attention.fc_out.weight\n",
      "crossatt.attention.fc_out.bias\n",
      "crossatt.norm.weight\n",
      "crossatt.norm.bias\n",
      "moose_encoder.motion_model.fnet.conv1.weight\n",
      "moose_encoder.motion_model.fnet.conv1.bias\n",
      "moose_encoder.motion_model.fnet.layer1.0.conv1.weight\n",
      "moose_encoder.motion_model.fnet.layer1.0.conv1.bias\n",
      "moose_encoder.motion_model.fnet.layer1.0.conv2.weight\n",
      "moose_encoder.motion_model.fnet.layer1.0.conv2.bias\n",
      "moose_encoder.motion_model.fnet.layer1.1.conv1.weight\n",
      "moose_encoder.motion_model.fnet.layer1.1.conv1.bias\n",
      "moose_encoder.motion_model.fnet.layer1.1.conv2.weight\n",
      "moose_encoder.motion_model.fnet.layer1.1.conv2.bias\n",
      "moose_encoder.motion_model.fnet.layer2.0.conv1.weight\n",
      "moose_encoder.motion_model.fnet.layer2.0.conv1.bias\n",
      "moose_encoder.motion_model.fnet.layer2.0.conv2.weight\n",
      "moose_encoder.motion_model.fnet.layer2.0.conv2.bias\n",
      "moose_encoder.motion_model.fnet.layer2.0.downsample.0.weight\n",
      "moose_encoder.motion_model.fnet.layer2.0.downsample.0.bias\n",
      "moose_encoder.motion_model.fnet.layer2.1.conv1.weight\n",
      "moose_encoder.motion_model.fnet.layer2.1.conv1.bias\n",
      "moose_encoder.motion_model.fnet.layer2.1.conv2.weight\n",
      "moose_encoder.motion_model.fnet.layer2.1.conv2.bias\n",
      "moose_encoder.motion_model.fnet.layer3.0.conv1.weight\n",
      "moose_encoder.motion_model.fnet.layer3.0.conv1.bias\n",
      "moose_encoder.motion_model.fnet.layer3.0.conv2.weight\n",
      "moose_encoder.motion_model.fnet.layer3.0.conv2.bias\n",
      "moose_encoder.motion_model.fnet.layer3.0.downsample.0.weight\n",
      "moose_encoder.motion_model.fnet.layer3.0.downsample.0.bias\n",
      "moose_encoder.motion_model.fnet.layer3.1.conv1.weight\n",
      "moose_encoder.motion_model.fnet.layer3.1.conv1.bias\n",
      "moose_encoder.motion_model.fnet.layer3.1.conv2.weight\n",
      "moose_encoder.motion_model.fnet.layer3.1.conv2.bias\n",
      "moose_encoder.motion_model.fnet.conv2.weight\n",
      "moose_encoder.motion_model.fnet.conv2.bias\n",
      "moose_encoder.motion_model.cnet.norm1.weight\n",
      "moose_encoder.motion_model.cnet.norm1.bias\n",
      "moose_encoder.motion_model.cnet.conv1.weight\n",
      "moose_encoder.motion_model.cnet.conv1.bias\n",
      "moose_encoder.motion_model.cnet.layer1.0.conv1.weight\n",
      "moose_encoder.motion_model.cnet.layer1.0.conv1.bias\n",
      "moose_encoder.motion_model.cnet.layer1.0.conv2.weight\n",
      "moose_encoder.motion_model.cnet.layer1.0.conv2.bias\n",
      "moose_encoder.motion_model.cnet.layer1.0.norm1.weight\n",
      "moose_encoder.motion_model.cnet.layer1.0.norm1.bias\n",
      "moose_encoder.motion_model.cnet.layer1.0.norm2.weight\n",
      "moose_encoder.motion_model.cnet.layer1.0.norm2.bias\n",
      "moose_encoder.motion_model.cnet.layer1.1.conv1.weight\n",
      "moose_encoder.motion_model.cnet.layer1.1.conv1.bias\n",
      "moose_encoder.motion_model.cnet.layer1.1.conv2.weight\n",
      "moose_encoder.motion_model.cnet.layer1.1.conv2.bias\n",
      "moose_encoder.motion_model.cnet.layer1.1.norm1.weight\n",
      "moose_encoder.motion_model.cnet.layer1.1.norm1.bias\n",
      "moose_encoder.motion_model.cnet.layer1.1.norm2.weight\n",
      "moose_encoder.motion_model.cnet.layer1.1.norm2.bias\n",
      "moose_encoder.motion_model.cnet.layer2.0.conv1.weight\n",
      "moose_encoder.motion_model.cnet.layer2.0.conv1.bias\n",
      "moose_encoder.motion_model.cnet.layer2.0.conv2.weight\n",
      "moose_encoder.motion_model.cnet.layer2.0.conv2.bias\n",
      "moose_encoder.motion_model.cnet.layer2.0.norm1.weight\n",
      "moose_encoder.motion_model.cnet.layer2.0.norm1.bias\n",
      "moose_encoder.motion_model.cnet.layer2.0.norm2.weight\n",
      "moose_encoder.motion_model.cnet.layer2.0.norm2.bias\n",
      "moose_encoder.motion_model.cnet.layer2.0.norm3.weight\n",
      "moose_encoder.motion_model.cnet.layer2.0.norm3.bias\n",
      "moose_encoder.motion_model.cnet.layer2.0.downsample.0.weight\n",
      "moose_encoder.motion_model.cnet.layer2.0.downsample.0.bias\n",
      "moose_encoder.motion_model.cnet.layer2.1.conv1.weight\n",
      "moose_encoder.motion_model.cnet.layer2.1.conv1.bias\n",
      "moose_encoder.motion_model.cnet.layer2.1.conv2.weight\n",
      "moose_encoder.motion_model.cnet.layer2.1.conv2.bias\n",
      "moose_encoder.motion_model.cnet.layer2.1.norm1.weight\n",
      "moose_encoder.motion_model.cnet.layer2.1.norm1.bias\n",
      "moose_encoder.motion_model.cnet.layer2.1.norm2.weight\n",
      "moose_encoder.motion_model.cnet.layer2.1.norm2.bias\n",
      "moose_encoder.motion_model.cnet.layer3.0.conv1.weight\n",
      "moose_encoder.motion_model.cnet.layer3.0.conv1.bias\n",
      "moose_encoder.motion_model.cnet.layer3.0.conv2.weight\n",
      "moose_encoder.motion_model.cnet.layer3.0.conv2.bias\n",
      "moose_encoder.motion_model.cnet.layer3.0.norm1.weight\n",
      "moose_encoder.motion_model.cnet.layer3.0.norm1.bias\n",
      "moose_encoder.motion_model.cnet.layer3.0.norm2.weight\n",
      "moose_encoder.motion_model.cnet.layer3.0.norm2.bias\n",
      "moose_encoder.motion_model.cnet.layer3.0.norm3.weight\n",
      "moose_encoder.motion_model.cnet.layer3.0.norm3.bias\n",
      "moose_encoder.motion_model.cnet.layer3.0.downsample.0.weight\n",
      "moose_encoder.motion_model.cnet.layer3.0.downsample.0.bias\n",
      "moose_encoder.motion_model.cnet.layer3.1.conv1.weight\n",
      "moose_encoder.motion_model.cnet.layer3.1.conv1.bias\n",
      "moose_encoder.motion_model.cnet.layer3.1.conv2.weight\n",
      "moose_encoder.motion_model.cnet.layer3.1.conv2.bias\n",
      "moose_encoder.motion_model.cnet.layer3.1.norm1.weight\n",
      "moose_encoder.motion_model.cnet.layer3.1.norm1.bias\n",
      "moose_encoder.motion_model.cnet.layer3.1.norm2.weight\n",
      "moose_encoder.motion_model.cnet.layer3.1.norm2.bias\n",
      "moose_encoder.motion_model.cnet.conv2.weight\n",
      "moose_encoder.motion_model.cnet.conv2.bias\n",
      "moose_encoder.motion_model.update_block.encoder.convc1.weight\n",
      "moose_encoder.motion_model.update_block.encoder.convc1.bias\n",
      "moose_encoder.motion_model.update_block.encoder.convc2.weight\n",
      "moose_encoder.motion_model.update_block.encoder.convc2.bias\n",
      "moose_encoder.motion_model.update_block.encoder.convf1.weight\n",
      "moose_encoder.motion_model.update_block.encoder.convf1.bias\n",
      "moose_encoder.motion_model.update_block.encoder.convf2.weight\n",
      "moose_encoder.motion_model.update_block.encoder.convf2.bias\n",
      "moose_encoder.motion_model.update_block.encoder.conv.weight\n",
      "moose_encoder.motion_model.update_block.encoder.conv.bias\n",
      "moose_encoder.motion_model.update_block.gru.convz1.weight\n",
      "moose_encoder.motion_model.update_block.gru.convz1.bias\n",
      "moose_encoder.motion_model.update_block.gru.convr1.weight\n",
      "moose_encoder.motion_model.update_block.gru.convr1.bias\n",
      "moose_encoder.motion_model.update_block.gru.convq1.weight\n",
      "moose_encoder.motion_model.update_block.gru.convq1.bias\n",
      "moose_encoder.motion_model.update_block.gru.convz2.weight\n",
      "moose_encoder.motion_model.update_block.gru.convz2.bias\n",
      "moose_encoder.motion_model.update_block.gru.convr2.weight\n",
      "moose_encoder.motion_model.update_block.gru.convr2.bias\n",
      "moose_encoder.motion_model.update_block.gru.convq2.weight\n",
      "moose_encoder.motion_model.update_block.gru.convq2.bias\n",
      "moose_encoder.motion_model.update_block.flow_head.conv1.weight\n",
      "moose_encoder.motion_model.update_block.flow_head.conv1.bias\n",
      "moose_encoder.motion_model.update_block.flow_head.conv2.weight\n",
      "moose_encoder.motion_model.update_block.flow_head.conv2.bias\n",
      "moose_encoder.motion_model.update_block.mask.0.weight\n",
      "moose_encoder.motion_model.update_block.mask.0.bias\n",
      "moose_encoder.motion_model.update_block.mask.2.weight\n",
      "moose_encoder.motion_model.update_block.mask.2.bias\n",
      "moose_encoder.visual_model.cls_token\n",
      "moose_encoder.visual_model.pos_embed\n",
      "moose_encoder.visual_model.patch_embed.proj.weight\n",
      "moose_encoder.visual_model.patch_embed.proj.bias\n",
      "moose_encoder.visual_model.blocks.0.norm1.weight\n",
      "moose_encoder.visual_model.blocks.0.norm1.bias\n",
      "moose_encoder.visual_model.blocks.0.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.0.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.0.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.0.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.0.norm2.weight\n",
      "moose_encoder.visual_model.blocks.0.norm2.bias\n",
      "moose_encoder.visual_model.blocks.0.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.0.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.0.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.0.mlp.fc2.bias\n",
      "moose_encoder.visual_model.blocks.1.norm1.weight\n",
      "moose_encoder.visual_model.blocks.1.norm1.bias\n",
      "moose_encoder.visual_model.blocks.1.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.1.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.1.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.1.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.1.norm2.weight\n",
      "moose_encoder.visual_model.blocks.1.norm2.bias\n",
      "moose_encoder.visual_model.blocks.1.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.1.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.1.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.1.mlp.fc2.bias\n",
      "moose_encoder.visual_model.blocks.2.norm1.weight\n",
      "moose_encoder.visual_model.blocks.2.norm1.bias\n",
      "moose_encoder.visual_model.blocks.2.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.2.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.2.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.2.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.2.norm2.weight\n",
      "moose_encoder.visual_model.blocks.2.norm2.bias\n",
      "moose_encoder.visual_model.blocks.2.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.2.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.2.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.2.mlp.fc2.bias\n",
      "moose_encoder.visual_model.blocks.3.norm1.weight\n",
      "moose_encoder.visual_model.blocks.3.norm1.bias\n",
      "moose_encoder.visual_model.blocks.3.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.3.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.3.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.3.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.3.norm2.weight\n",
      "moose_encoder.visual_model.blocks.3.norm2.bias\n",
      "moose_encoder.visual_model.blocks.3.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.3.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.3.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.3.mlp.fc2.bias\n",
      "moose_encoder.visual_model.blocks.4.norm1.weight\n",
      "moose_encoder.visual_model.blocks.4.norm1.bias\n",
      "moose_encoder.visual_model.blocks.4.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.4.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.4.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.4.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.4.norm2.weight\n",
      "moose_encoder.visual_model.blocks.4.norm2.bias\n",
      "moose_encoder.visual_model.blocks.4.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.4.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.4.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.4.mlp.fc2.bias\n",
      "moose_encoder.visual_model.blocks.5.norm1.weight\n",
      "moose_encoder.visual_model.blocks.5.norm1.bias\n",
      "moose_encoder.visual_model.blocks.5.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.5.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.5.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.5.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.5.norm2.weight\n",
      "moose_encoder.visual_model.blocks.5.norm2.bias\n",
      "moose_encoder.visual_model.blocks.5.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.5.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.5.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.5.mlp.fc2.bias\n",
      "moose_encoder.visual_model.blocks.6.norm1.weight\n",
      "moose_encoder.visual_model.blocks.6.norm1.bias\n",
      "moose_encoder.visual_model.blocks.6.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.6.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.6.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.6.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.6.norm2.weight\n",
      "moose_encoder.visual_model.blocks.6.norm2.bias\n",
      "moose_encoder.visual_model.blocks.6.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.6.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.6.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.6.mlp.fc2.bias\n",
      "moose_encoder.visual_model.blocks.7.norm1.weight\n",
      "moose_encoder.visual_model.blocks.7.norm1.bias\n",
      "moose_encoder.visual_model.blocks.7.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.7.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.7.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.7.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.7.norm2.weight\n",
      "moose_encoder.visual_model.blocks.7.norm2.bias\n",
      "moose_encoder.visual_model.blocks.7.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.7.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.7.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.7.mlp.fc2.bias\n",
      "moose_encoder.visual_model.blocks.8.norm1.weight\n",
      "moose_encoder.visual_model.blocks.8.norm1.bias\n",
      "moose_encoder.visual_model.blocks.8.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.8.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.8.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.8.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.8.norm2.weight\n",
      "moose_encoder.visual_model.blocks.8.norm2.bias\n",
      "moose_encoder.visual_model.blocks.8.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.8.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.8.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.8.mlp.fc2.bias\n",
      "moose_encoder.visual_model.blocks.9.norm1.weight\n",
      "moose_encoder.visual_model.blocks.9.norm1.bias\n",
      "moose_encoder.visual_model.blocks.9.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.9.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.9.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.9.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.9.norm2.weight\n",
      "moose_encoder.visual_model.blocks.9.norm2.bias\n",
      "moose_encoder.visual_model.blocks.9.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.9.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.9.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.9.mlp.fc2.bias\n",
      "moose_encoder.visual_model.blocks.10.norm1.weight\n",
      "moose_encoder.visual_model.blocks.10.norm1.bias\n",
      "moose_encoder.visual_model.blocks.10.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.10.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.10.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.10.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.10.norm2.weight\n",
      "moose_encoder.visual_model.blocks.10.norm2.bias\n",
      "moose_encoder.visual_model.blocks.10.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.10.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.10.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.10.mlp.fc2.bias\n",
      "moose_encoder.visual_model.blocks.11.norm1.weight\n",
      "moose_encoder.visual_model.blocks.11.norm1.bias\n",
      "moose_encoder.visual_model.blocks.11.attn.qkv.weight\n",
      "moose_encoder.visual_model.blocks.11.attn.qkv.bias\n",
      "moose_encoder.visual_model.blocks.11.attn.proj.weight\n",
      "moose_encoder.visual_model.blocks.11.attn.proj.bias\n",
      "moose_encoder.visual_model.blocks.11.norm2.weight\n",
      "moose_encoder.visual_model.blocks.11.norm2.bias\n",
      "moose_encoder.visual_model.blocks.11.mlp.fc1.weight\n",
      "moose_encoder.visual_model.blocks.11.mlp.fc1.bias\n",
      "moose_encoder.visual_model.blocks.11.mlp.fc2.weight\n",
      "moose_encoder.visual_model.blocks.11.mlp.fc2.bias\n",
      "moose_encoder.visual_model.norm.weight\n",
      "moose_encoder.visual_model.norm.bias\n",
      "patch_embed.proj.weight\n",
      "patch_embed.proj.bias\n",
      "head.weight\n",
      "head.bias\n"
     ]
    }
   ],
   "source": [
    "for name, p in moose.named_parameters():\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sapiens_lite",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
